# KNN com Pondera√ß√£o de Dist√¢ncias

Diferentemente do KNN simples, onde cada vizinho tem o mesmo peso na decis√£o, o KNN com pondera√ß√£o de dist√¢ncias atribui pesos aos vizinhos com base na proximidade do ponto de teste. Isso significa que vizinhos mais pr√≥ximos ter√£o maior influ√™ncia na decis√£o do que vizinhos mais distantes.

Essa abordagem permite um modelo mais flex√≠vel e menos sens√≠vel √† escolha do par√¢metro ùëò, tornando a classifica√ß√£o mais robusta.

O c√≥digo fornecido neste reposit√≥rio implementa essa vers√£o do KNN, permitindo comparar a fronteira de decis√£o com a do modelo simples.

## Atividade

- **Objetivos:** Representa√ß√£o do KNN com pondera√ß√£o de dist√¢ncias e entendimento desta abordagem como uma combina√ß√£o de fun√ß√µes radiais.
- **Tarefa:** Estender a implementa√ß√£o anterior incluindo a pondera√ß√£o de dist√¢ncias para a tomada de decis√£o da classifica√ß√£o. De acordo com esta abordagem a resposta do KNN √© representada na forma

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  $$\hat{y} = \text{sinal} \left( \sum_{i=1}^{N} \alpha_i y_i K(x, x_i) \right)$$
  
  em que $\alpha_i = 1 \ \forall x_i \in  V_k$, $\alpha_i = 0 \ \forall x_i \notin  V_k$, $V_k$ √© o conjunto dos vizinhos mais pr√≥ximos e $K(x, x_i)$ √© um fun√ß√£o de kernel radial na forma de uma fun√ß√£o Gaussiana.

    Avaliar os efeitos dos par√¢metros do modelo e da fun√ß√£o no desempenho do modelo, considerando a resposta observada da superf√≠cie de separa√ß√£o. Para o exemplo de dados sint√©ticos implementado, a fun√ß√£o Gaussiana (Normal) √© de duas vari√°veis, no entanto, sugere-se que ela seja implementada em sua forma geral:

    \begin{equation}
        p(x) = \frac{1}{\sqrt{(2\pi)^n |\boldsymbol{\Sigma}|}} \exp \left( -\frac{1}{2} (x - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (x - \boldsymbol{\mu}) \right)
    \end{equation}

    em que $n$ √© a dimens√£o de $x$, $\boldsymbol{\Sigma}$ √© a matriz de covari√¢ncias, $|\boldsymbol{\Sigma}|$ o seu determinante e $\boldsymbol{\mu}$ √© o vetor de m√©dias das distribui√ß√µes marginais, conforme apresentado a seguir nas Equa√ß√µes~\ref{eq:covarianca} e~\ref{eq:dist_marg}.

    \begin{equation}
        \boldsymbol{\Sigma} =
        \begin{bmatrix}
            \sigma_1^2 & \rho_{12} \sigma_1 \sigma_2 & \cdots & \rho_{1n} \sigma_1 \sigma_n \\
            \rho_{21} \sigma_2 \sigma_1 & \sigma_2^2 & \cdots & \rho_{2n} \sigma_2 \sigma_n \\
            \vdots & \vdots & \ddots & \vdots \\
            \rho_{n1} \sigma_n \sigma_1 & \rho_{n2} \sigma_n \sigma_2 & \cdots & \sigma_n^2
            \label{eq:covarianca}
        \end{bmatrix}
    \end{equation}

    \begin{equation}
        \boldsymbol{\mu} =
        \begin{bmatrix}
            \mu_1 \\
            \mu_2 \\
            \vdots \\
            \mu_n
        \end{bmatrix}
        \label{eq:dist_marg}
    \end{equation}

    Para facilitar a implementa√ß√£o, considere a seguinte implementa√ß√£o em $R$ da fun√ß√£o de densidade Normal multivariada:

    \begin{lstlisting}[language=R]
        pdfnvar <- function(x, m, K, n) (1 / (sqrt((2 * pi)^n * det(K)))) * exp(-0.5 * (t(x - m) %*% solve(K) %*% (x - m)))
    \end{lstlisting}

    em que $\boldsymbol{K} = h\boldsymbol{I}$, sendo assim $h$ o raio da fun√ß√£o Gaussiana.

    Nesta etapa o desempenho do classificador ser√° avaliado de acordo com os par√¢metros $k$ e $h$.
